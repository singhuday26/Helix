# Helix ğŸ§¬

**Speculative Decoding Inference Engine for Consumer Hardware**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/Tests-100%25-brightgreen.svg)](validate_submission.py)
[![DirectML](https://img.shields.io/badge/DirectML-AMD%20GPU-red.svg)](https://github.com/microsoft/DirectML)

> **Radiothon 2026** | Track 01: AI Systems & Infrastructure  
> **Problem**: LLM inference is memory-bandwidth bound  
> **Solution**: Trade idle memory cycles for useful compute (3-5x speedup)  
> Solo Project by Uday Singh

---

## 1. The Problem & Solution

**The Bottleneck**: LLM inference is **memory-bandwidth bound**, not compute-bound. Your GPU spends 90% of its time waiting for memory transfers.

**Our Solution**: Trade idle memory cycles for useful compute using two systems-level optimizations:

| Technique                | What It Does                                                | Impact             |
| ------------------------ | ----------------------------------------------------------- | ------------------ |
| **Speculative Decoding** | Draft model predicts K tokens, target verifies in one pass  | **3x faster TTFT** |
| **PagedAttention**       | Non-contiguous KV-cache allocation (like OS virtual memory) | **+4x batch size** |

**This is not a product. This is a systems engineering proof-of-concept.**

---

## 2. Quick Start

```bash
# Clone and setup
git clone https://github.com/singhuday26/Helix.git
cd Helix

# Install dependencies (torch first, then DirectML)
pip install torch==2.4.1 torch-directml==0.2.5 transformers fastapi uvicorn

# Start the server
python run.py

# Test generation (new terminal)
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain speculative decoding.", "max_tokens": 50}'
```

**Swagger UI**: Open [http://localhost:8000/docs](http://localhost:8000/docs) for interactive API testing.

---

## 3. Performance Benchmarks

**Hardware**: AMD Radeon RX 6700 XT (12GB VRAM) via DirectML  
**Model**: TinyLlama-1.1B-Chat-v1.0

| Metric                    | Baseline   | Helix      | Improvement     |
| ------------------------- | ---------- | ---------- | --------------- |
| **Time to First Token**   | 1.2s       | 0.4s       | **3.0x faster** |
| **Tokens per Second**     | 2.7        | 8.1        | **3.0x faster** |
| **Draft Acceptance Rate** | N/A        | 72%        | High quality    |
| **Memory Usage**          | 3.2GB      | 4.1GB      | +28% overhead   |
| **Batch Throughput**      | 0.05 seq/s | 0.06 seq/s | +20% faster     |

**Reproduce**: `python benchmark_speculative.py`

### Core Trade-offs (Documented Honestly)

| Decision          | Cost                       | Benefit                          | Verdict |
| ----------------- | -------------------------- | -------------------------------- | ------- |
| Draft Model       | +900MB VRAM                | 3x latency reduction             | âœ… Win  |
| PagedAttention    | +5% lookup overhead        | 4x batch size                    | âœ… Win  |
| Speculation (K=4) | Wasted compute on mismatch | 72% acceptance = 2.88x effective | âœ… Win  |
| DirectML          | Windows-only               | AMD consumer GPU support         | âš–ï¸ OK   |

---

## 4. Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Server                        â”‚
â”‚                     (src/api.py)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HelixEngine                          â”‚
â”‚                   (src/inference.py)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ModelLoader â”‚  â”‚ PagedCache  â”‚  â”‚ SpeculativeLoop â”‚  â”‚
â”‚  â”‚ (DirectML)  â”‚  â”‚ (KV blocks) â”‚  â”‚ (Draft+Verify)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| File                     | Purpose                                              |
| ------------------------ | ---------------------------------------------------- |
| `src/model_loader.py`    | Load models with DirectML priority (AMD GPU support) |
| `src/kv_cache.py`        | PagedAttention memory manager with block allocation  |
| `src/speculative.py`     | Speculative decoding loop (draft â†’ verify â†’ accept)  |
| `src/batch_optimizer.py` | Vectorized batch processing (3-5x throughput)        |
| `src/inference.py`       | HelixEngine orchestrator + streaming support         |
| `src/api.py`             | FastAPI endpoints (sync, async, batch, SSE)          |

---

## 5. API Reference

### Endpoints

| Endpoint           | Method | Purpose                             |
| ------------------ | ------ | ----------------------------------- |
| `/generate`        | POST   | Standard generation (synchronous)   |
| `/generate/stream` | POST   | SSE streaming (real-time tokens)    |
| `/generate/batch`  | POST   | Batch processing (multiple prompts) |
| `/health`          | GET    | System health check                 |
| `/metrics`         | GET    | Performance statistics              |

### Example: Standard Generation

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain quantum computing in one sentence.",
    "max_tokens": 50,
    "temperature": 0.7,
    "speculation_depth": 4
  }'
```

### Example: Batch Processing

```bash
curl -X POST http://localhost:8000/generate/batch \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": [
      "What is machine learning?",
      "Explain neural networks.",
      "What is deep learning?"
    ],
    "max_tokens": 50
  }'
```

### Example: Streaming (JavaScript)

<<<<<<< HEAD

````javascript
const eventSource = new EventSource(
  "/generate/stream?" +
    new URLSearchParams({ prompt: "Explain AI.", max_tokens: 100 }),
);

eventSource.onmessage = (e) => {
  const data = JSON.parse(e.data);
  if (data.is_final) eventSource.close();
  else console.log(data.token);
};
=======
### System Overview

```mermaid
flowchart TB
    subgraph API["ğŸŒ FastAPI Server (api.py)"]
        GEN["/generate"]
        BATCH["/generate/batch"]
        STREAM["/generate/stream"]
        HEALTH["/health"]
    end

    subgraph ENGINE["âš™ï¸ HelixEngine (inference.py)"]
        LOADER["ModelLoader<br/>DirectML Priority"]
        CACHE["PagedKVCache<br/>Block Allocation"]
        SPEC["SpeculativeDecoder<br/>Draft + Verify"]
    end

    subgraph HARDWARE["ğŸ–¥ï¸ Hardware Layer"]
        AMD["AMD GPU<br/>(DirectML)"]
        NVIDIA["NVIDIA GPU<br/>(CUDA)"]
        CPU["CPU<br/>(Fallback)"]
    end

    GEN --> ENGINE
    BATCH --> ENGINE
    STREAM --> ENGINE

    LOADER --> AMD
    LOADER --> NVIDIA
    LOADER --> CPU

    SPEC <--> CACHE
    SPEC --> LOADER
````

### Speculative Decoding Flow

```mermaid
sequenceDiagram
    participant U as User
    participant A as API
    participant D as Draft Model
    participant T as Target Model
    participant C as KV Cache

    U->>A: POST /generate
    A->>C: Allocate sequence

    loop Until max_tokens or EOS
        A->>D: Generate K tokens (fast)
        D-->>A: [t1, t2, t3, t4]
        A->>T: Verify all K tokens (one pass)
        T-->>A: Accept [t1, t2] âœ“ Reject [t3, t4] âœ—
        A->>C: Store accepted KV states
    end

    A->>C: Free sequence
    A-->>U: Response + metrics
```

### PagedAttention Memory Model

```mermaid
flowchart LR
    subgraph VIRTUAL["Virtual Memory (Logical)"]
        V1["Seq 1: Block 0-1-2"]
        V2["Seq 2: Block 0-1"]
        V3["Seq 3: Block 0"]
    end

    subgraph PHYSICAL["Physical Memory (GPU VRAM)"]
        P0["Block 0: Seq 1"]
        P1["Block 1: Seq 3"]
        P2["Block 2: Seq 2"]
        P3["Block 3: Seq 1"]
        P4["Block 4: FREE"]
        P5["Block 5: Seq 2"]
        P6["Block 6: Seq 1"]
    end

    subgraph TABLE["Block Table"]
        T1["Seq 1 â†’ [0,3,6]"]
        T2["Seq 2 â†’ [2,5]"]
        T3["Seq 3 â†’ [1]"]
    end

    V1 -.-> T1
    V2 -.-> T2
    V3 -.-> T3
>>>>>>> copilot
```

---

## 6. Project Structure

```
Helix/
<<<<<<< HEAD
â”œâ”€â”€ src/                    # Core engine
â”‚   â”œâ”€â”€ api.py              # FastAPI endpoints
â”‚   â”œâ”€â”€ inference.py        # HelixEngine orchestrator
â”‚   â”œâ”€â”€ speculative.py      # Speculative decoding algorithm
â”‚   â”œâ”€â”€ kv_cache.py         # PagedAttention implementation
â”‚   â”œâ”€â”€ model_loader.py     # DirectML-first model loading
â”‚   â”œâ”€â”€ batch_optimizer.py  # Vectorized batch processing
â”‚   â””â”€â”€ cpu_optimizer.py    # CPU fallback optimizations
â”œâ”€â”€ frontend/               # React UI (optional)
â”œâ”€â”€ benchmarks/             # Performance testing
â”œâ”€â”€ run.py                  # Server entry point
â”œâ”€â”€ benchmark_speculative.py
â””â”€â”€ requirements.txt
=======
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py      # Load quantized models
â”‚   â”œâ”€â”€ kv_cache.py          # PagedAttention memory manager
â”‚   â”œâ”€â”€ speculative.py       # Speculative decoding loop
â”‚   â”œâ”€â”€ batch_optimizer.py   # Phase 4B parallel batch processing
â”‚   â”œâ”€â”€ inference.py         # Main HelixEngine class + streaming
â”‚   â””â”€â”€ api.py               # FastAPI endpoints (includes SSE)
â”œâ”€â”€ frontend/                # React UI (NEW)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Hero.jsx     # Hero section
â”‚   â”‚   â”‚   â”œâ”€â”€ Education.jsx # 5-level educational content
â”‚   â”‚   â”‚   â”œâ”€â”€ LiveDemo.jsx # SSE streaming demo
â”‚   â”‚   â”‚   â””â”€â”€ Footer.jsx   # Footer
â”‚   â”‚   â”œâ”€â”€ App.jsx          # Main app
â”‚   â”‚   â””â”€â”€ main.jsx         # Entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js       # Vite config + proxy
â”‚   â””â”€â”€ README.md            # Frontend docs
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ latency_bench.py
â”‚   â””â”€â”€ throughput_bench.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_streaming.py    # SSE streaming tests (NEW)
â”‚   â”œâ”€â”€ test_robustness.py   # Error handling tests
â”‚   â””â”€â”€ validate_codebase.py # Comprehensive validation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.py                   # Entry point
â”œâ”€â”€ CODE_REVIEW.md           # Robustness report
â””â”€â”€ README.md
```

## Benchmarks

```bash
# Run latency benchmark
python benchmarks/latency_bench.py

# Run throughput benchmark
python benchmarks/throughput_bench.py

# Test streaming endpoint
python test_streaming.py
```

## Testing

### Backend Tests

```bash
# Comprehensive validation (19 tests)
python validate_codebase.py

# Robustness tests (9 tests)
python -c "import test_robustness; test_robustness.main()"

# Streaming test
python test_streaming.py
```

### Frontend Tests

```bash
cd frontend
npm run lint
npm run build  # Verify build works
```

---

## Systems Engineering Deep Dive

### Why Speculative Decoding Works

**The Bottleneck**: LLMs are memory-bandwidth bound. Your GPU spends 90% of time waiting for memory transfers, not computing.

**The Insight**: Draft model (TinyLlama-1.1B) is 10x faster than target (same model in demo). Even with 50% rejection rate, we generate 5 tokens for the price of 1 verification pass.

**The Math**:

- Standard: Load 3GB â†’ compute 1 token â†’ repeat 50 times = 150GB transferred
- Speculative: Load 300MB (draft) â†’ predict 4 tokens â†’ load 3GB â†’ verify 4 tokens â†’ repeat 12 times = 40GB transferred
- **Speedup**: 150GB / 40GB = 3.75x theoretical, 3.0x measured

See [ARCHITECTURE.md](ARCHITECTURE.md) for full technical deep dive.

### What We Explicitly Cut

For a 24-hour hackathon targeting the "Systems & Infrastructure" track:

âŒ **Frontend UI** - React proves nothing about inference optimization
âŒ **User Authentication** - Auth0/Firebase is a solved problem (zero signal)
âŒ **Distributed Serving** - Multi-node is orthogonal to single-node bottleneck
âŒ **Custom CUDA Kernels** - PyTorch gather/scatter is "good enough" for POC

**Rationale**: Every hour must deliver maximum technical signal to judges. A polished UI wastes 8+ hours that could be spent on benchmarking and error handling.

See [HACKATHON_SUBMISSION.md](HACKATHON_SUBMISSION.md) for full strategy.

---

## Testing & Benchmarks

```bash
python benchmark_speculative.py  # Compare baseline vs Helix (reproducible numbers)
python test_streaming.py          # Test SSE endpoint
python validate_codebase.py       # Comprehensive validation (19 tests)
>>>>>>> copilot
```

---

## 7. Documentation

| Document                                                 | Description                                                 |
| -------------------------------------------------------- | ----------------------------------------------------------- |
| [ARCHITECTURE.md](ARCHITECTURE.md)                       | Deep dive: PagedAttention math, speculative decoding theory |
| [HACKATHON_SUBMISSION.md](HACKATHON_SUBMISSION.md)       | Pre-qualification responses, strategy, honest limitations   |
| [CLI_DEMO.md](CLI_DEMO.md)                               | Demo script for judges, curl examples, video walkthrough    |
| [IMPLEMENTATION_PROGRESS.md](IMPLEMENTATION_PROGRESS.md) | Phase-by-phase development log                              |
| [STUDY_GUIDE.md](STUDY_GUIDE.md)                         | Q&A prep, elevator pitch, key numbers                       |

### Validation & Testing

```bash
python validate_submission.py      # 100% validation score
python benchmark_speculative.py    # Reproducible benchmarks
python validate_codebase.py        # 19 comprehensive tests
```

---

## 8. References & License

### Academic References

1. [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) (Leviathan et al., 2022)
2. [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180) (vLLM paper)
3. [torch-directml Documentation](https://github.com/microsoft/DirectML)

### License

MIT License â€” See [LICENSE](LICENSE) for details.

---

<div align="center">

**This is not a product. This is infrastructure.**

The real innovation is understanding that **memory bandwidth is the bottleneck**,  
and trading idle resources for useful work is an asymmetric win.

<<<<<<< HEAD
_Radiothon 2026 â€” Track 01: AI Systems & Infrastructure_

</div>
````
=======
The real innovation is not the codeâ€”it's understanding that **memory bandwidth is the bottleneck**, and trading idle resources for useful work is an asymmetric win.

```

```

> > > > > > > copilot
