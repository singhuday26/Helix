# Helix ğŸ§¬

**Speculative Decoding Inference Engine for Edge Devices**

> Built for Radiothon 2026 | Track 01: AI Systems & Infrastructure

---

## What is Helix?

Helix is a lightweight LLM inference engine that demonstrates **Senior Engineer** architectural thinking through:

1. **Speculative Decoding** â€” Uses a small draft model to predict tokens speculatively, then verifies with target model in a single forward pass
2. **PagedAttention** â€” Non-contiguous KV-cache allocation to reduce memory fragmentation and increase batch throughput

## Key Trade-offs

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Draft Model | TinyLlama-1.1B | Fastest speculation, fits in L2 cache |
| Memory Strategy | Paged (vs Contiguous) | +4x batch size, +~5% latency overhead |
| Consistency | Eventual (async verify) | Prioritize TTFT over strict ordering |

---

## Quick Start

```bash
# 1. Create environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# 2. Install dependencies
pip install -r requirements.txt

# 3. Start the server
python run.py

# 4. Open Swagger docs
# Navigate to http://localhost:8000/docs
```

## API Usage

```bash
# Generate text
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain quantum computing in one sentence.", "max_tokens": 50}'
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Server                      â”‚
â”‚                     (src/api.py)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HelixEngine                           â”‚
â”‚                  (src/inference.py)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ModelLoader â”‚  â”‚ PagedCache  â”‚  â”‚ SpeculativeLoop â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
Helix/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py    # Load quantized models
â”‚   â”œâ”€â”€ kv_cache.py        # PagedAttention memory manager
â”‚   â”œâ”€â”€ speculative.py     # Speculative decoding loop
â”‚   â”œâ”€â”€ inference.py       # Main HelixEngine class
â”‚   â””â”€â”€ api.py             # FastAPI endpoints
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ latency_bench.py
â”‚   â””â”€â”€ throughput_bench.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.py                 # Entry point
â””â”€â”€ README.md
```

## Benchmarks

```bash
# Run latency benchmark
python benchmarks/latency_bench.py

# Run throughput benchmark
python benchmarks/throughput_bench.py
```

---

## License

MIT â€” Built for Radiothon 2026
